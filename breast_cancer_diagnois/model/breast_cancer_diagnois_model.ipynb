{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1764346467594,
     "user": {
      "displayName": "Huy Truương",
      "userId": "12891518826596935690"
     },
     "user_tz": -420
    },
    "id": "8QVWB4yNnLlq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1764344777323,
     "user": {
      "displayName": "Huy Truương",
      "userId": "12891518826596935690"
     },
     "user_tz": -420
    },
    "id": "IrMh7a0KqWe3"
   },
   "outputs": [],
   "source": [
    "def get_clean_data():\n",
    "    data = pd.read_csv(\"data/data.csv\")\n",
    "    data = data.drop(['Unnamed: 32', 'id'], axis=1)\n",
    "    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6pKNbnJGJfd"
   },
   "source": [
    "Đọc tệp data.csv: Đây là bước cần thiết vì nó tải dữ liệu gốc mà ta sẽ làm việc lên môi trường lập trình. Tệp này chứa thông tin về các trường hợp ung thư vú.\n",
    "\n",
    "- Loại bỏ các cột 'Unnamed: 32' và 'id':\n",
    "\n",
    "    - Cột 'Unnamed: 32' thường xuất hiện khi có một cột trống không có tiêu đề trong tệp CSV, nó không chứa thông tin hữu ích cho phân tích hoặc mô hình.\n",
    "    - Cột 'id' chứa một định danh duy nhất cho mỗi bản ghi. Mặc dù quan trọng để xác định các trường hợp riêng lẻ, nhưng nó không phải là một đặc trưng (feature) giúp dự đoán ung thư. Do đó, việc loại bỏ nó giúp mô hình không bị nhầm lẫn và giảm nhiễu.\n",
    "\n",
    "- Ánh xạ cột 'diagnosis' từ {'M': 1, 'B': 0}:\n",
    "\n",
    "    - Cột 'diagnosis' ban đầu chứa các giá trị chuỗi là 'M' (Malignant - Ác tính) và 'B' (Benign - Lành tính). Hầu hết các thuật toán học máy đều hoạt động với dữ liệu số.\n",
    "    - Việc chuyển đổi 'M' thành 1 và 'B' thành 0 giúp biểu diễn các loại chẩn đoán này dưới dạng số mà mô hình có thể dễ dàng xử lý. Đây là một bước chuẩn bị dữ liệu quan trọng để huấn luyện mô hình phân loại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1764345322862,
     "user": {
      "displayName": "Huy Truương",
      "userId": "12891518826596935690"
     },
     "user_tz": -420
    },
    "id": "4JhDIy2pqqS2"
   },
   "outputs": [],
   "source": [
    "def create_model(data):\n",
    "    X = data.drop(['diagnosis'], axis=1)\n",
    "    y = data['diagnosis']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #train\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return  model, scaler, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYNGyLAnJ5A4"
   },
   "source": [
    "1. Tách Đặc trưng và Biến Mục tiêu: Đầu tiên, nó tách dữ liệu đầu vào (data) thành các đặc trưng (X), là tất cả các cột trừ 'diagnosis', và biến mục tiêu (y), chính là cột 'diagnosis'.\n",
    "\n",
    "2. Chuẩn hóa Đặc trưng: Nó khởi tạo một StandardScaler và áp dụng nó cho các đặc trưng (X). Bước này chuẩn hóa các đặc trưng bằng cách loại bỏ giá trị trung bình và chia tỷ lệ để có phương sai đơn vị. Điều này rất quan trọng đối với nhiều thuật toán học máy, bao gồm cả Hồi quy Logistic, để hoạt động tối ưu.\n",
    "\n",
    "3. Chia tập huấn luyện và kiểm tra: Sau đó, nó chia các đặc trưng đã được chuẩn hóa (X) và biến mục tiêu (y) thành các tập huấn luyện và kiểm tra bằng cách sử dụng train_test_split. 80% dữ liệu được sử dụng để huấn luyện (X_train, y_train) và 20% để kiểm tra (X_test, y_test), với random_state=42 đảm bảo khả năng tái tạo của việc chia tách.\n",
    "\n",
    "4. Huấn luyện Mô hình: Một mô hình LogisticRegression được khởi tạo và sau đó được huấn luyện (fit) bằng cách sử dụng dữ liệu huấn luyện (X_train, y_train). Đây là nơi mô hình học mối quan hệ giữa các đặc trưng và chẩn đoán.\n",
    "\n",
    "5. Trả về Giá trị: Cuối cùng, hàm trả về model đã được huấn luyện và đối tượng scaler. Đối tượng scaler được trả về vì nó sẽ cần thiết để biến đổi dữ liệu mới, chưa từng thấy theo cùng một cách mà dữ liệu huấn luyện đã được biến đổi trước khi đưa ra dự đoán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1764345374749,
     "user": {
      "displayName": "Huy Truương",
      "userId": "12891518826596935690"
     },
     "user_tz": -420
    },
    "id": "QbrBNOEhJo54"
   },
   "outputs": [],
   "source": [
    "def test_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyrBR8j4RiA-"
   },
   "source": [
    "`test_model`, chịu trách nhiệm đánh giá hiệu suất của mô hình học máy đã được huấn luyện.\n",
    "\n",
    "Dưới đây là chi tiết về những gì nó thực hiện:\n",
    "\n",
    "1. Dự đoán: y_pred = model.predict(X_test): Dòng này sử dụng model đã được huấn luyện để đưa ra các dự đoán (y_pred) trên các đặc trưng kiểm tra chưa từng thấy (X_test). Những dự đoán này là phỏng đoán tốt nhất của mô hình về 'chẩn đoán' dựa trên các đặc trưng đầu vào.\n",
    "2. Tính độ chính xác: accuracy = accuracy_score(y_test, y_pred): Điều này tính toán độ chính xác tổng thể của mô hình bằng cách so sánh các nhãn thực tế (y_test) với các dự đoán của mô hình (y_pred). Độ chính xác là tỷ lệ các trường hợp được phân loại đúng.\n",
    "3. In độ chính xác: print(\"Accuracy:\", accuracy): Điều này hiển thị độ chính xác đã tính toán ra console.\n",
    "4. Tạo Báo cáo Phân loại: print(\"Classification Report:\\n\", classification_report(y_test, y_pred)): Điều này tạo và in một báo cáo phân loại chi tiết. Báo cáo này cung cấp các chỉ số sắc thái hơn ngoài độ chính xác, chẳng hạn như độ chính xác (precision), độ thu hồi (recall) và điểm f1-score cho mỗi lớp (Ác tính và Lành tính trong trường hợp này), cùng với số lượng hỗ trợ (số lần xuất hiện thực tế của mỗi lớp trong tập kiểm tra). Điều này giúp hiểu hiệu suất của mô hình cho từng danh mục."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1764346968915,
     "user": {
      "displayName": "Huy Truương",
      "userId": "12891518826596935690"
     },
     "user_tz": -420
    },
    "id": "aWXeEtq4PycA"
   },
   "outputs": [],
   "source": [
    "def dump_model(model,scaler):\n",
    "    with open('model/model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open('model/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOekEl3_Q-in"
   },
   "source": [
    "`dump_model` nhận đầu vào là model (mô hình học máy đã được huấn luyện) và đối tượng scaler. Mục đích của nó là lưu hai đối tượng này vào các tệp bằng cách sử dụng module pickle của Python.\n",
    "\n",
    "- Nó mở một tệp có tên 'model/model.pkl' ở chế độ ghi nhị phân ('wb') và sử dụng pickle.dump() để lưu đối tượng model vào tệp này.\n",
    "- Tương tự, nó mở một tệp khác có tên 'model/scaler.pkl' ở chế độ ghi nhị phân ('wb') và sử dụng pickle.dump() để lưu đối tượng scaler vào tệp này.\n",
    "\n",
    "Quá trình này rất cần thiết cho việc duy trì: một khi mô hình của bạn đã được huấn luyện và bộ chia tỷ lệ dữ liệu của bạn đã được khớp, bạn có thể lưu chúng lại. Điều này cho phép bạn tải chúng sau này để đưa ra dự đoán trên dữ liệu mới mà không cần phải huấn luyện lại mô hình hoặc khớp lại bộ chia tỷ lệ mỗi lần."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1764346717515,
     "user": {
      "displayName": "Huy Truương",
      "userId": "12891518826596935690"
     },
     "user_tz": -420
    },
    "id": "eBoSp4u3LJX4",
    "outputId": "85ac9262-8faf-4b34-c55b-915137198a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98        71\n",
      "           1       0.98      0.95      0.96        43\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data = get_clean_data()\n",
    "    model, scaler, X_test, y_test = create_model(data)\n",
    "    test_model(model, X_test, y_test)\n",
    "\n",
    "    dump_model(model, scaler)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPSAMQzdINIBVM+NOkuDtD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
